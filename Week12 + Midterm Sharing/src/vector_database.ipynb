{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6387c0e8-5461-44f9-a7ef-79279bb956f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.15.0 in /opt/conda/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow==2.15.0\n",
    "%pip install spacy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134ca67-3d68-4d7d-aebf-bb3ca04d08d2",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b50c9e-ce33-42b1-84d4-731ed10ee1db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 09:37:58.519753: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-08 09:37:59.403735: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-08 09:37:59.403847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-08 09:37:59.604441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-08 09:38:00.007334: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-08 09:38:00.011817: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578ac57-2625-4c73-a96c-c597f05086e8",
   "metadata": {},
   "source": [
    "#### Setup Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01374b62-d624-4d53-add4-7f56c9b7f0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@keras.utils.register_keras_serializable(package=\"CustomModels\")\n",
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=512, embed_dim=32, num_heads=2, ff_dim=32, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_tags': self.num_tags,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'maxlen': self.maxlen,\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "        }\n",
    "        base_config = super(NERModel, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings\n",
    "    \n",
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, reduction='sum', name=\"custom_ner_loss\"):\n",
    "        super().__init__(reduction='sum', name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction=self.reduction\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep=\"\\t\")\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1 : length + 1]\n",
    "    tags = record[length + 1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return tokens, tags\n",
    "\n",
    "def lookup(tokens):\n",
    "    # Load the list from the file\n",
    "    with open('vocabulary.pkl', 'rb') as f:\n",
    "        loaded_list = pickle.load(f)\n",
    "    # The StringLookup class will convert tokens to token IDs\n",
    "    lookup_layer = keras.layers.StringLookup(vocabulary=loaded_list)\n",
    "\n",
    "    # No need to lowercase Vietnamese characters\n",
    "    return lookup_layer(tokens)\n",
    "\n",
    "def format_datatype(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    #tokens =  [re.sub(r'[;,]', '', d) for d in data.split(' ')]\n",
    "    #default is 0, since is for prediction\n",
    "    ner_tags = [0 for d in data.split(' ')]\n",
    "\n",
    "    #tab to separate\n",
    "    string_input = str(len(tokens))+ \"\\t\"+ \"\\t\".join(tokens)+ \"\\t\"+ \"\\t\".join(map(str, ner_tags))\n",
    "    string_input = tf.data.Dataset.from_tensor_slices([string_input])\n",
    "\n",
    "\n",
    "    finalize_input = (string_input.map(map_record_to_training_data)\n",
    "                      .map(lambda x, y: (lookup(x),  y))\n",
    "                      .padded_batch(1)\n",
    "                      )\n",
    "\n",
    "    return finalize_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c6454-1069-4d79-b128-7e232bbce1bb",
   "metadata": {},
   "source": [
    "#### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c401671e-333c-4e9a-88b1-90eb910ee433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.get_custom_objects()['CustomNonPaddingTokenLoss'] = CustomNonPaddingTokenLoss\n",
    "# Load model\n",
    "loaded_model = tf.keras.models.load_model(\"ner_model.keras\")\n",
    "\n",
    "embedding_layer = loaded_model.embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c6d0c-293e-4be5-92e1-575c0f42307a",
   "metadata": {},
   "source": [
    "#### Load Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59e25f9-7f0c-4432-a32b-ff5639e2d112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_bq(project_id, dataset_id, table_id, bigquery_client):\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {project_id}.{dataset_id}.{table_id}\n",
    "    \"\"\"\n",
    "    query_job = bigquery_client.query(query)\n",
    "    df = query_job.to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7268a74e-b7a1-4630-af02-1e151d9a7375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JLR_LINK</th>\n",
       "      <th>TRANS_TYPE_OF_CASE</th>\n",
       "      <th>TRANS_LEGAL_RELATIONSHIP</th>\n",
       "      <th>PDF_TEXT</th>\n",
       "      <th>EXTRACT</th>\n",
       "      <th>ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>BIRTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://congbobanan.toaan.gov.vn/2ta1016089t1c...</td>\n",
       "      <td>刑事</td>\n",
       "      <td>盜竊財產罪</td>\n",
       "      <td>&lt;Page:1&gt;TÒA ÁN NHÂN DÂN HUYỆN BẮC HÀ CỘNG HÒA ...</td>\n",
       "      <td>1. Sùng Seo Q, sinh ngày 13/7/2003 tại huyện ...</td>\n",
       "      <td>No_Id</td>\n",
       "      <td>Sùng Seo Q</td>\n",
       "      <td>2003</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>2003-07-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://congbobanan.toaan.gov.vn/2ta791888t1cv...</td>\n",
       "      <td>刑事</td>\n",
       "      <td>犯罪賭博</td>\n",
       "      <td>&lt;Page:1&gt;1\\nTÒA ÁN NHÂN DÂN CỘNG HÒA XÃ HỘI CHỦ...</td>\n",
       "      <td>1. Đỗ Đình H1, sinh ngày 20/6/1961 tại thôn H...</td>\n",
       "      <td>No_Id</td>\n",
       "      <td>Vũ Văn Th</td>\n",
       "      <td>1959</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>Male</td>\n",
       "      <td>1959-03-08 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://congbobanan.toaan.gov.vn/2ta536274t1cv...</td>\n",
       "      <td>採取行政處理措施的決定</td>\n",
       "      <td>放入強制性戒毒機構</td>\n",
       "      <td>&lt;Page:1&gt;TOÀ ÁN NHÂN DÂN CỘNG HOÀ XÃ HỘI CHỦ NG...</td>\n",
       "      <td>Họ và tên: Nguyễn Văn H. Giới tính: Nam; Sinh...</td>\n",
       "      <td>No_Id</td>\n",
       "      <td>Nguyễn Văn H</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-02-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://congbobanan.toaan.gov.vn/2ta290704t1cv...</td>\n",
       "      <td>採取行政處理措施的決定</td>\n",
       "      <td>放入強制性戒毒機構</td>\n",
       "      <td>&lt;Page:1&gt;TÒA ÁN NHÂN DÂN CỘNG HÒA XÃ HỘI CHỦ NG...</td>\n",
       "      <td>Họ và tên: Nguyễn Văn T; Giới T: Nam; sinh ng...</td>\n",
       "      <td>No_Id</td>\n",
       "      <td>Nguyễn Văn T</td>\n",
       "      <td>1970</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>1970-09-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://congbobanan.toaan.gov.vn/2ta946636t1cv...</td>\n",
       "      <td>刑事</td>\n",
       "      <td>犯罪組織賭博或持賭</td>\n",
       "      <td>&lt;Page:1&gt;TÒA ÁN NHÂN DÂN CỘNG HÒA XÃ HỘI CHỦ NG...</td>\n",
       "      <td>1. Nguyễn Kim P, (tên gọi khác: Không), sinh ...</td>\n",
       "      <td>No_Id</td>\n",
       "      <td>Nguyễn Thu H</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>Female</td>\n",
       "      <td>1992-01-19 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            JLR_LINK TRANS_TYPE_OF_CASE  \\\n",
       "0  https://congbobanan.toaan.gov.vn/2ta1016089t1c...                 刑事   \n",
       "1  https://congbobanan.toaan.gov.vn/2ta791888t1cv...                 刑事   \n",
       "2  https://congbobanan.toaan.gov.vn/2ta536274t1cv...        採取行政處理措施的決定   \n",
       "3  https://congbobanan.toaan.gov.vn/2ta290704t1cv...        採取行政處理措施的決定   \n",
       "4  https://congbobanan.toaan.gov.vn/2ta946636t1cv...                 刑事   \n",
       "\n",
       "  TRANS_LEGAL_RELATIONSHIP                                           PDF_TEXT  \\\n",
       "0                    盜竊財產罪  <Page:1>TÒA ÁN NHÂN DÂN HUYỆN BẮC HÀ CỘNG HÒA ...   \n",
       "1                     犯罪賭博  <Page:1>1\\nTÒA ÁN NHÂN DÂN CỘNG HÒA XÃ HỘI CHỦ...   \n",
       "2                放入強制性戒毒機構  <Page:1>TOÀ ÁN NHÂN DÂN CỘNG HOÀ XÃ HỘI CHỦ NG...   \n",
       "3                放入強制性戒毒機構  <Page:1>TÒA ÁN NHÂN DÂN CỘNG HÒA XÃ HỘI CHỦ NG...   \n",
       "4                犯罪組織賭博或持賭  <Page:1>TÒA ÁN NHÂN DÂN CỘNG HÒA XÃ HỘI CHỦ NG...   \n",
       "\n",
       "                                             EXTRACT     ID          NAME  \\\n",
       "0   1. Sùng Seo Q, sinh ngày 13/7/2003 tại huyện ...  No_Id    Sùng Seo Q   \n",
       "1   1. Đỗ Đình H1, sinh ngày 20/6/1961 tại thôn H...  No_Id     Vũ Văn Th   \n",
       "2   Họ và tên: Nguyễn Văn H. Giới tính: Nam; Sinh...  No_Id  Nguyễn Văn H   \n",
       "3   Họ và tên: Nguyễn Văn T; Giới T: Nam; sinh ng...  No_Id  Nguyễn Văn T   \n",
       "4   1. Nguyễn Kim P, (tên gọi khác: Không), sinh ...  No_Id  Nguyễn Thu H   \n",
       "\n",
       "   Year  Month  Day  GENDER                BIRTH  \n",
       "0  2003      7   13    Male  2003-07-13 00:00:00  \n",
       "1  1959      3    8    Male  1959-03-08 00:00:00  \n",
       "2  1991      2   16    Male  1991-02-16 00:00:00  \n",
       "3  1970      9   13    Male  1970-09-13 00:00:00  \n",
       "4  1992      1   19  Female  1992-01-19 00:00:00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_ID, DATASET_ID, TABLE_ID = \"intern-project-415606\", \"Criminal_Dataset\", \"criminal_data\"\n",
    "bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
    "dataset = read_bq(PROJECT_ID, DATASET_ID, TABLE_ID, bigquery_client)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ca7f3-ec27-495e-8e8e-fcaf5d15131e",
   "metadata": {},
   "source": [
    "#### Build vector database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac295f6-9b6d-4d73-897a-b7ca0ed3c20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"JLR_LINK\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"NAME\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"VECTOR\", \"STRING\", mode=\"REQUIRED\")\n",
    "    ]\n",
    "    project_id, dataset_id, table_id_write = 'intern-project-415606', 'Criminal_Dataset', 'criminal_name_vector'\n",
    "    table = bigquery.Table(f\"{project_id}.{dataset_id}.{table_id_write}\", schema=schema)\n",
    "    try:\n",
    "        table = bigquery_client.create_table(table)\n",
    "    except Exception as e:\n",
    "        print(f\"Table {project_id}.{dataset_id}.{table_id_write} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eceae406-b1b9-419d-b31c-f9f42ec80e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_and_insert(name, jlr_link):\n",
    "    embedding_vectors = embedding_layer(lookup([name]))\n",
    "\n",
    "    embedding_vector_list = embedding_vectors.numpy().tolist()[0]\n",
    "    \n",
    "    return embedding_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85fc3d-c6bd-4c0d-a509-f21ed33a803d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table intern-project-415606.Criminal_Dataset.criminal_name_vector already exists.\n"
     ]
    }
   ],
   "source": [
    "create_table()\n",
    "names = dataset['NAME']\n",
    "jlr_links = dataset['JLR_LINK']\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "for i in range(32654, len(names)):\n",
    "    vectors = transform_and_insert(names[i], jlr_links[i])\n",
    "    query = f\"\"\"\n",
    "        INSERT INTO `intern-project-415606.Criminal_Dataset`.criminal_name_vector (`JLR_LINK`, `NAME`, `VECTOR`)\n",
    "        VALUES ('{jlr_links[i]}', '{names[i]}', '{str(vectors)}')\n",
    "    \"\"\"\n",
    "    client.query(query)\n",
    "    if i % 500 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c45d8-d930-4740-bd4b-32aa30ad38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
